{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = config['EMR']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = config['EMR']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    When running spark script in command line mode, we need to\n",
    "    initiate spark session on our own. While, in jupyter notebook,\n",
    "    it's launched in the background automatically.\n",
    "\n",
    "    :return: spark session obj\n",
    "    \"\"\"\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = \"s3a://tom-dend-bucket-615\"  # s3 bucket of data source\n",
    "output_data = \"s3a://tom-dend-bucket-918\"  # s3 bucket of my own bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Read data from S3.\n"
     ]
    }
   ],
   "source": [
    "# get filepath to song data file\n",
    "stage_song_pth = f\"{input_data}/song_data/*/*/*/*.json\"\n",
    "# read song data file\n",
    "df = spark.read.json(stage_song_pth)  # TODO: 確認讀進song_data時，可以用wild-card match\n",
    "print(\"Success: Read data from S3.\")\n",
    "\n",
    "# extract columns to create songs table\n",
    "songs_table = df.select(\n",
    "    \"song_id\", \"title\", \"artist_id\",\n",
    "    \"year\", \"duration\").dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Wrote song_table to parquet format.\n"
     ]
    }
   ],
   "source": [
    "songs_table.write.parquet(\n",
    "    path=f\"{output_data}/song_table\",\n",
    "    mode=\"overwrite\",\n",
    "    partitionBy=[\"year\", \"artist_id\"]\n",
    ")\n",
    "print(\"Success: Wrote song_table to parquet format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns to create artists table\n",
    "artists_table = df.select(\n",
    "    \"artist_id\", \"artist_name\", \"artist_location\",\n",
    "    \"artist_latitude\", \"artist_longitude\").dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Wrote artist_table to parquet format.\n"
     ]
    }
   ],
   "source": [
    "# write artists table to parquet files\n",
    "artists_table.write.parquet(\n",
    "    path=f\"{output_data}/artist_table\",\n",
    "    mode=\"overwrite\"\n",
    ")\n",
    "print(\"Success: Wrote artist_table to parquet format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filepath to log data file\n",
    "stage_log_data_pth = f\"{input_data}/log-data/*.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Read data from S3.\n"
     ]
    }
   ],
   "source": [
    "# read log data file\n",
    "df = spark.read.json(stage_log_data_pth)\n",
    "print(\"Success: Read data from S3.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Filter log_data on NextSong.\n"
     ]
    }
   ],
   "source": [
    "# filter by actions for song plays\n",
    "# only select actions of \"NextSong\"\n",
    "df = df.filter(df[\"page\"] == \"NextSong\")\n",
    "print(\"Success: Filter log_data on NextSong.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns for users table\n",
    "users_table = df.select(\n",
    "    \"userId\", \"firstName\", \"lastName\",\n",
    "    \"gender\", \"level\").dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Wrote user_table to parquet format.\n"
     ]
    }
   ],
   "source": [
    "# write users table to parquet files\n",
    "users_table.write.parquet(\n",
    "    path=f\"{output_data}/user_table\",\n",
    "    mode=\"overwrite\"\n",
    ")\n",
    "print(\"Success: Wrote user_table to parquet format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Create column start_time from ts.\n"
     ]
    }
   ],
   "source": [
    "# create timestamp column from original timestamp column\n",
    "df = df.withColumn(\"start_time\", F.from_unixtime(F.col(\"ts\")/1000))\n",
    "print(\"Success: Create column start_time from ts.\")\n",
    "\n",
    "# extract columns to create time table\n",
    "time_table = df.select(\"ts\", \"start_time\") \\\n",
    "    .withColumn(\"hour\", F.hour(\"start_time\")) \\\n",
    "    .withColumn(\"day\", F.dayofmonth(\"start_time\")) \\\n",
    "    .withColumn(\"week\", F.weekofyear(\"start_time\")) \\\n",
    "    .withColumn(\"month\", F.month(\"start_time\")) \\\n",
    "    .withColumn(\"year\", F.year(\"start_time\")) \\\n",
    "    .withColumn(\"weekday\", F.dayofweek(\"start_time\")) \\\n",
    "    .dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns to create time table\n",
    "time_table = df.select(\"ts\", \"start_time\") \\\n",
    "    .withColumn(\"hour\", F.hour(\"start_time\")) \\\n",
    "    .withColumn(\"day\", F.dayofmonth(\"start_time\")) \\\n",
    "    .withColumn(\"week\", F.weekofyear(\"start_time\")) \\\n",
    "    .withColumn(\"month\", F.month(\"start_time\")) \\\n",
    "    .withColumn(\"year\", F.year(\"start_time\")) \\\n",
    "    .withColumn(\"weekday\", F.dayofweek(\"start_time\")) \\\n",
    "    .dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Wrote time_table to parquet format.\n"
     ]
    }
   ],
   "source": [
    "# write time table to parquet files partitioned by year and month\n",
    "time_table.write.parquet(\n",
    "    path=f\"{output_data}/time_table\",\n",
    "    mode=\"overwrite\",\n",
    "    partitionBy=[\"year\", \"month\"]\n",
    ")\n",
    "print(\"Success: Wrote time_table to parquet format.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Read in song_table from partitioned parquet file.\n"
     ]
    }
   ],
   "source": [
    "song_table_pth = f\"{output_data}/song_table/*/*/*\"\n",
    "song_table = spark.read.parquet(song_table_pth)\n",
    "print(\"Success: Read in song_table from partitioned parquet file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Read in artist_table from non-partitioned parquet file.\n"
     ]
    }
   ],
   "source": [
    "# read in artist_table\n",
    "artist_table_pth = f\"{output_data}/artist_table/*\"\n",
    "artists_table = spark.read.parquet(artist_table_pth)\n",
    "print(\"Success: Read in artist_table from non-partitioned parquet file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create temp view\n",
    "song_table.createOrReplaceTempView(\"song_table\")\n",
    "time_table.createOrReplaceTempView(\"time_table\")\n",
    "users_table.createOrReplaceTempView(\"user_table\")\n",
    "artists_table.createOrReplaceTempView(\"artist_table\")\n",
    "df.createOrReplaceTempView(\"stage_events_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "artists_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songs_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      " |-- start_time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Process sql query.\n"
     ]
    }
   ],
   "source": [
    "# extract columns from joined song and log data sets to create songplays table\n",
    "songplays_table = spark.sql(\"\"\"\n",
    "    select distinct\n",
    "        t.start_time,\n",
    "        t.year,\n",
    "        t.month,\n",
    "        u.userId,\n",
    "        e.level,\n",
    "        s.song_id,\n",
    "        a.artist_id,\n",
    "        e.sessionId,\n",
    "        e.userAgent\n",
    "    from\n",
    "        stage_events_table as e\n",
    "    inner join time_table t on t.ts = e.ts \n",
    "    inner join user_table u on e.userId = u.userId\n",
    "    inner join song_table s on s.title = e.song\n",
    "    inner join artist_table a on a.artist_name = e.artist\n",
    "\"\"\")\n",
    "print(\"Success: Process sql query.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Wrote songplay_table to parquet format.\n"
     ]
    }
   ],
   "source": [
    "# write songplays table to parquet files partitioned by year and month\n",
    "songplays_table.write.parquet(\n",
    "    path=f\"{output_data}/songplay_table\",\n",
    "    mode=\"overwrite\",\n",
    "    partitionBy=[\"year\", \"month\"]\n",
    ")\n",
    "print(\"Success: Wrote songplay_table to parquet format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
